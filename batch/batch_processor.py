import threading
import time
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import List, Callable, Any, Dict, Optional

from api.ark_api_utils import RateLimiter
from .config import BatchConfig


class ProgressTracker:
    """è¿›åº¦è·Ÿè¸ªå™¨"""
    def __init__(self, total_tasks: int):
        self.total_tasks = total_tasks
        self.completed_tasks = 0
        self.failed_tasks = 0
        self.lock = threading.Lock()
        self.start_time = time.time()
    
    def update_progress(self, success: bool = True):
        """æ›´æ–°è¿›åº¦"""
        with self.lock:
            if success:
                self.completed_tasks += 1
            else:
                self.failed_tasks += 1
            
            total_finished = self.completed_tasks + self.failed_tasks
            progress = (total_finished / self.total_tasks) * 100
            elapsed_time = time.time() - self.start_time
            
            if total_finished > 0:
                avg_time_per_task = elapsed_time / total_finished
                remaining_tasks = self.total_tasks - total_finished
                estimated_remaining_time = avg_time_per_task * remaining_tasks
                
                print(f"ğŸ“Š è¿›åº¦: {total_finished}/{self.total_tasks} ({progress:.1f}%) "
                      f"âœ…æˆåŠŸ: {self.completed_tasks} | âŒå¤±è´¥: {self.failed_tasks} "
                      f"â±ï¸é¢„è®¡å‰©ä½™: {estimated_remaining_time:.1f}ç§’")
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        elapsed_time = time.time() - self.start_time
        return {
            'total_tasks': self.total_tasks,
            'completed_tasks': self.completed_tasks,
            'failed_tasks': self.failed_tasks,
            'success_rate': (self.completed_tasks / self.total_tasks * 100) if self.total_tasks > 0 else 0,
            'elapsed_time': elapsed_time,
            'tasks_per_second': self.completed_tasks / elapsed_time if elapsed_time > 0 else 0
        }


def execute_single_task_with_retry(
    task_func: Callable,
    task_data: Any,
    config: BatchConfig,
    rate_limiter: RateLimiter,
    progress_tracker: ProgressTracker
) -> Dict[str, Any]:
    """
    æ‰§è¡Œå•ä¸ªä»»åŠ¡ï¼ˆå¸¦é‡è¯•æœºåˆ¶ï¼‰
    
    Args:
        task_func: è¦æ‰§è¡Œçš„ä»»åŠ¡å‡½æ•°
        task_data: ä¼ é€’ç»™ä»»åŠ¡å‡½æ•°çš„æ•°æ®
        config: æ‰¹é‡å¤„ç†é…ç½®
        rate_limiter: é€Ÿç‡é™åˆ¶å™¨
        progress_tracker: è¿›åº¦è·Ÿè¸ªå™¨
    
    Returns:
        ä»»åŠ¡æ‰§è¡Œç»“æœå­—å…¸
    """
    result = {
        'success': False,
        'data': None,
        'error': None,
        'attempts': 0,
        'task_data': task_data
    }
    
    for attempt in range(config.max_retries):
        try:
            result['attempts'] = attempt + 1
            
            # é€Ÿç‡é™åˆ¶æ£€æŸ¥
            rate_limiter.wait_if_needed()
            
            # æ‰§è¡Œä»»åŠ¡ï¼ˆå¸¦è¶…æ—¶ï¼‰
            start_time = time.time()
            task_result = task_func(task_data)
            execution_time = time.time() - start_time
            
            # æ£€æŸ¥è¶…æ—¶
            if execution_time > config.timeout:
                raise TimeoutError(f"ä»»åŠ¡æ‰§è¡Œè¶…æ—¶: {execution_time:.2f}ç§’ > {config.timeout}ç§’")
            
            result['success'] = True
            result['data'] = task_result
            result['execution_time'] = execution_time
            
            progress_tracker.update_progress(success=True)
            return result
            
        except Exception as e:
            result['error'] = str(e)
            
            if attempt < config.max_retries - 1:
                print(f"âš ï¸ ä»»åŠ¡ç¬¬{attempt + 1}æ¬¡å°è¯•å¤±è´¥ï¼Œå‡†å¤‡é‡è¯•: {e}")
                time.sleep(config.retry_delay)
            else:
                print(f"âŒ ä»»åŠ¡é‡è¯•{config.max_retries}æ¬¡åä»å¤±è´¥: {e}")
    
    progress_tracker.update_progress(success=False)
    return result


def parallel_batch_processor(
    tasks: List[Any],
    task_func: Callable[[Any], Any],
    config: Optional[BatchConfig] = None,
    show_progress: bool = True
) -> Dict[str, Any]:
    """
    å¹¶è¡Œæ‰¹é‡å¤„ç†å™¨
    
    è¿™æ˜¯ä¸€ä¸ªé€šç”¨çš„å¹¶è¡Œæ‰¹é‡å¤„ç†å·¥å…·ï¼Œå¯ä»¥å¤„ç†ä»»ä½•ç±»å‹çš„ä»»åŠ¡ã€‚
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨ï¼Œæ¯ä¸ªå…ƒç´ å°†ä½œä¸ºå‚æ•°ä¼ é€’ç»™task_func
        task_func: ä»»åŠ¡å¤„ç†å‡½æ•°ï¼Œæ¥æ”¶å•ä¸ªä»»åŠ¡æ•°æ®ä½œä¸ºå‚æ•°
        config: æ‰¹é‡å¤„ç†é…ç½®ï¼Œå¦‚æœä¸ºNoneåˆ™ä½¿ç”¨é»˜è®¤é…ç½®
        show_progress: æ˜¯å¦æ˜¾ç¤ºè¿›åº¦ä¿¡æ¯
    
    Returns:
        åŒ…å«å¤„ç†ç»“æœå’Œç»Ÿè®¡ä¿¡æ¯çš„å­—å…¸:
        {
            'results': List[Dict],  # æ¯ä¸ªä»»åŠ¡çš„è¯¦ç»†ç»“æœ
            'stats': Dict,          # ç»Ÿè®¡ä¿¡æ¯
            'successful_results': List,  # æˆåŠŸä»»åŠ¡çš„ç»“æœæ•°æ®
            'failed_tasks': List    # å¤±è´¥ä»»åŠ¡çš„ä¿¡æ¯
        }
    
    Example:
        # ç¤ºä¾‹1: æ‰¹é‡HTTPè¯·æ±‚
        def http_request(url):
            import requests
            response = requests.get(url, timeout=10)
            return response.json()
        
        urls = ['http://api1.com', 'http://api2.com', 'http://api3.com']
        config = BatchConfig(max_workers=5, rate_limit_per_minute=120)
        result = parallel_batch_processor(urls, http_request, config)
        
        # ç¤ºä¾‹2: æ‰¹é‡æ–‡ä»¶å¤„ç†
        def process_file(filepath):
            with open(filepath, 'r') as f:
                return len(f.read())
        
        files = ['file1.txt', 'file2.txt', 'file3.txt']
        result = parallel_batch_processor(files, process_file)
        
        # ç¤ºä¾‹3: æ‰¹é‡æ•°æ®è½¬æ¢
        def transform_data(item):
            return item * 2 + 1
        
        numbers = list(range(100))
        result = parallel_batch_processor(numbers, transform_data)
    """
    
    if config is None:
        config = BatchConfig()
    
    if not tasks:
        return {
            'results': [],
            'stats': {'total_tasks': 0, 'completed_tasks': 0, 'failed_tasks': 0},
            'successful_results': [],
            'failed_tasks': []
        }
    
    if show_progress:
        print(f"ğŸš€ å¼€å§‹å¹¶è¡Œæ‰¹é‡å¤„ç†ï¼Œä»»åŠ¡æ•°: {len(tasks)}")
        print(f"âš¡ é…ç½®: æœ€å¤§å¹¶å‘{config.max_workers} | é€Ÿç‡é™åˆ¶{config.rate_limit_per_minute}/åˆ†é’Ÿ | æœ€å¤§é‡è¯•{config.max_retries}æ¬¡")
        print("=" * 60)
    
    # åˆ›å»ºè¿›åº¦è·Ÿè¸ªå™¨å’Œé€Ÿç‡é™åˆ¶å™¨
    progress_tracker = ProgressTracker(len(tasks))
    rate_limiter = RateLimiter(max_requests=config.rate_limit_per_minute, window_seconds=config.rate_limit_window)
    
    all_results = []
    
    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶å‘æ‰§è¡Œä»»åŠ¡
    with ThreadPoolExecutor(max_workers=config.max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_task = {}
        for i, task_data in enumerate(tasks):
            # æ·»åŠ è¯·æ±‚å»¶è¿Ÿï¼Œé¿å…è¿‡äºå¯†é›†çš„APIè°ƒç”¨
            if i > 0 and config.request_delay > 0:
                time.sleep(config.request_delay / config.max_workers)
            
            future = executor.submit(
                execute_single_task_with_retry,
                task_func,
                task_data,
                config,
                rate_limiter,
                progress_tracker
            )
            future_to_task[future] = task_data
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in as_completed(future_to_task):
            try:
                result = future.result()
                all_results.append(result)
            except Exception as e:
                # å¤„ç†æœªæ•è·çš„å¼‚å¸¸
                task_data = future_to_task[future]
                error_result = {
                    'success': False,
                    'data': None,
                    'error': f"æœªæ•è·å¼‚å¸¸: {str(e)}",
                    'attempts': 0,
                    'task_data': task_data
                }
                all_results.append(error_result)
                progress_tracker.update_progress(success=False)
    
    # ç”Ÿæˆæœ€ç»ˆç»Ÿè®¡å’Œç»“æœ
    stats = progress_tracker.get_stats()
    successful_results = [r['data'] for r in all_results if r['success']]
    failed_tasks = [r for r in all_results if not r['success']]
    
    if show_progress:
        print("=" * 60)
        print(f"ğŸ‰ æ‰¹é‡å¤„ç†å®Œæˆ!")
        print(f"ğŸ“Š ç»Ÿè®¡: æ€»ä»»åŠ¡{stats['total_tasks']}ä¸ª | âœ…æˆåŠŸ{stats['completed_tasks']}ä¸ª | âŒå¤±è´¥{stats['failed_tasks']}ä¸ª")
        print(f"ğŸ“ˆ æˆåŠŸç‡: {stats['success_rate']:.1f}%")
        print(f"â±ï¸ æ€»è€—æ—¶: {stats['elapsed_time']:.1f}ç§’")
        print(f"âš¡ å¤„ç†é€Ÿåº¦: {stats['tasks_per_second']:.2f}ä¸ª/ç§’")
        print(f"ğŸ“ æ€»APIè°ƒç”¨: {rate_limiter.total_api_calls}æ¬¡")
        
        if failed_tasks:
            print(f"âŒ å¤±è´¥ä»»åŠ¡è¯¦æƒ…:")
            for i, failed_task in enumerate(failed_tasks[:5], 1):  # åªæ˜¾ç¤ºå‰5ä¸ªå¤±è´¥ä»»åŠ¡
                print(f"   {i}. ä»»åŠ¡: {failed_task['task_data']} | é”™è¯¯: {failed_task['error']}")
            if len(failed_tasks) > 5:
                print(f"   ... è¿˜æœ‰ {len(failed_tasks) - 5} ä¸ªå¤±è´¥ä»»åŠ¡")
    
    return {
        'results': all_results,
        'stats': stats,
        'successful_results': successful_results,
        'failed_tasks': failed_tasks
    }


def simple_parallel_map(tasks: List[Any], task_func: Callable[[Any], Any], max_workers: int = 10) -> List[Any]:
    """
    ç®€åŒ–ç‰ˆå¹¶è¡Œå¤„ç†å™¨ - ç±»ä¼¼äºå†…ç½®mapå‡½æ•°çš„å¹¶è¡Œç‰ˆæœ¬
    
    Args:
        tasks: ä»»åŠ¡åˆ—è¡¨
        task_func: å¤„ç†å‡½æ•°
        max_workers: æœ€å¤§å¹¶å‘æ•°
    
    Returns:
        å¤„ç†ç»“æœåˆ—è¡¨ï¼ˆä¿æŒä¸è¾“å…¥ç›¸åŒçš„é¡ºåºï¼‰
    
    Example:
        # ç®€å•çš„å¹¶è¡Œå¤„ç†
        numbers = list(range(10))
        results = simple_parallel_map(numbers, lambda x: x ** 2, max_workers=4)
        print(results)  # [0, 1, 4, 9, 16, 25, 36, 49, 64, 81]
    """
    config = BatchConfig(max_workers=max_workers, max_retries=1, rate_limit_per_minute=max_workers * 60)
    result = parallel_batch_processor(tasks, task_func, config, show_progress=False)
    
    # åˆ›å»ºä¸€ä¸ªä»ä»»åŠ¡æ•°æ®åˆ°ç»“æœçš„æ˜ å°„
    task_map = {str(res['task_data']): res for res in result['results']}

    # æŒ‰åŸå§‹ä»»åŠ¡é¡ºåºæ„å»ºç»“æœåˆ—è¡¨
    ordered_results = []
    for task_data in tasks:
        task_result = task_map.get(str(task_data))
        if task_result and task_result['success']:
            ordered_results.append(task_result['data'])
        else:
            ordered_results.append(None)
            
    return ordered_results 